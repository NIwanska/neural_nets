{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open('test_no_target.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Calculate min and max values in the dataset\n",
    "all_sequences = [seq for seq, _ in train_data]\n",
    "flat_sequences = np.concatenate(all_sequences)\n",
    "min_val, max_val = flat_sequences.min(), flat_sequences.max()\n",
    "\n",
    "# Normalize sequences to be between 0 and 1\n",
    "def normalize_sequence(sequence, min_val, max_val):\n",
    "    return (sequence - min_val) / (max_val - min_val)\n",
    "\n",
    "train_data = [(normalize_sequence(seq, min_val, max_val), label) for seq, label in train_data]\n",
    "test_data = [normalize_sequence(seq, min_val, max_val) for seq in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.data[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float), label\n",
    "\n",
    "# Create dataset\n",
    "dataset = SequenceDataset(train_data)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Padding function\n",
    "def pad_collate(batch, pad_value=-1):\n",
    "    xx, yy = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    yy_pad = torch.tensor(yy)\n",
    "    return xx_pad, yy_pad, x_lens\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class LSTM_Seq_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM_Seq_Classifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths, hidden):\n",
    "        lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.lstm(x_packed, hidden)\n",
    "\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        out = output[torch.arange(output.size(0)), lengths - 1]\n",
    "\n",
    "        x = self.fc(out)\n",
    "        \n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTM_Seq_Classifier(input_size=1, hidden_size=128, num_layers=2, num_classes=5).to(device)\n",
    "class_weights = torch.tensor([0.05, 0.16, 0.5, 0.17, 0.3], device=device) \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/51], Loss: 1.4631, Training Accuracy: 36.36%, Validation Accuracy: 55.44%\n",
      "Epoch [2/51], Loss: 1.4635, Training Accuracy: 40.98%, Validation Accuracy: 45.92%\n",
      "Epoch [3/51], Loss: 1.3792, Training Accuracy: 46.57%, Validation Accuracy: 48.87%\n",
      "Epoch [4/51], Loss: 1.4454, Training Accuracy: 44.00%, Validation Accuracy: 44.56%\n",
      "Epoch [5/51], Loss: 1.1912, Training Accuracy: 49.44%, Validation Accuracy: 46.60%\n",
      "Epoch [6/51], Loss: 1.3577, Training Accuracy: 51.24%, Validation Accuracy: 51.59%\n",
      "Epoch [7/51], Loss: 1.5371, Training Accuracy: 52.84%, Validation Accuracy: 52.49%\n",
      "Epoch [8/51], Loss: 1.0982, Training Accuracy: 55.96%, Validation Accuracy: 60.09%\n",
      "Epoch [9/51], Loss: 1.4737, Training Accuracy: 57.02%, Validation Accuracy: 56.58%\n",
      "Epoch [10/51], Loss: 1.6633, Training Accuracy: 57.07%, Validation Accuracy: 59.30%\n",
      "Epoch [11/51], Loss: 0.7740, Training Accuracy: 59.02%, Validation Accuracy: 59.07%\n",
      "Epoch [12/51], Loss: 0.9959, Training Accuracy: 58.82%, Validation Accuracy: 58.96%\n",
      "Epoch [13/51], Loss: 1.2216, Training Accuracy: 62.13%, Validation Accuracy: 64.63%\n",
      "Epoch [14/51], Loss: 1.0995, Training Accuracy: 65.00%, Validation Accuracy: 58.73%\n",
      "Epoch [15/51], Loss: 0.7408, Training Accuracy: 58.48%, Validation Accuracy: 46.03%\n",
      "Epoch [16/51], Loss: 0.8604, Training Accuracy: 63.30%, Validation Accuracy: 59.86%\n",
      "Epoch [17/51], Loss: 0.4413, Training Accuracy: 63.64%, Validation Accuracy: 60.77%\n",
      "Epoch [18/51], Loss: 0.5604, Training Accuracy: 63.68%, Validation Accuracy: 59.86%\n",
      "Epoch [19/51], Loss: 1.0661, Training Accuracy: 65.58%, Validation Accuracy: 61.11%\n",
      "Epoch [20/51], Loss: 1.5004, Training Accuracy: 66.46%, Validation Accuracy: 61.79%\n",
      "Epoch [21/51], Loss: 0.5182, Training Accuracy: 66.99%, Validation Accuracy: 63.38%\n",
      "Epoch [22/51], Loss: 1.2488, Training Accuracy: 69.32%, Validation Accuracy: 63.83%\n",
      "Epoch [23/51], Loss: 0.9248, Training Accuracy: 70.00%, Validation Accuracy: 55.44%\n",
      "Epoch [24/51], Loss: 0.3249, Training Accuracy: 67.09%, Validation Accuracy: 63.27%\n",
      "Epoch [25/51], Loss: 0.7204, Training Accuracy: 71.27%, Validation Accuracy: 60.66%\n",
      "Epoch [26/51], Loss: 0.3985, Training Accuracy: 73.65%, Validation Accuracy: 65.08%\n",
      "Epoch [27/51], Loss: 0.4276, Training Accuracy: 75.84%, Validation Accuracy: 60.66%\n",
      "Epoch [28/51], Loss: 0.5045, Training Accuracy: 75.69%, Validation Accuracy: 66.89%\n",
      "Epoch [29/51], Loss: 0.4294, Training Accuracy: 76.67%, Validation Accuracy: 66.55%\n",
      "Epoch [30/51], Loss: 0.2381, Training Accuracy: 78.03%, Validation Accuracy: 67.91%\n",
      "Epoch [31/51], Loss: 0.6691, Training Accuracy: 79.87%, Validation Accuracy: 64.51%\n",
      "Epoch [32/51], Loss: 0.3149, Training Accuracy: 82.11%, Validation Accuracy: 64.40%\n",
      "Epoch [33/51], Loss: 0.0395, Training Accuracy: 83.28%, Validation Accuracy: 70.18%\n",
      "Epoch [34/51], Loss: 0.0650, Training Accuracy: 85.85%, Validation Accuracy: 68.82%\n",
      "Epoch [35/51], Loss: 0.2308, Training Accuracy: 85.71%, Validation Accuracy: 52.95%\n",
      "Epoch [36/51], Loss: 0.3610, Training Accuracy: 79.00%, Validation Accuracy: 67.69%\n",
      "Epoch [37/51], Loss: 0.1896, Training Accuracy: 86.14%, Validation Accuracy: 66.21%\n",
      "Epoch [38/51], Loss: 0.9450, Training Accuracy: 84.93%, Validation Accuracy: 62.81%\n",
      "Epoch [39/51], Loss: 0.5976, Training Accuracy: 72.97%, Validation Accuracy: 62.70%\n",
      "Epoch [40/51], Loss: 0.3044, Training Accuracy: 76.32%, Validation Accuracy: 70.29%\n",
      "Epoch [41/51], Loss: 0.1304, Training Accuracy: 85.08%, Validation Accuracy: 70.98%\n",
      "Epoch [42/51], Loss: 0.0919, Training Accuracy: 87.12%, Validation Accuracy: 68.37%\n",
      "Epoch [43/51], Loss: 0.1619, Training Accuracy: 88.92%, Validation Accuracy: 72.56%\n",
      "Epoch [44/51], Loss: 0.2377, Training Accuracy: 91.93%, Validation Accuracy: 72.79%\n",
      "Epoch [45/51], Loss: 0.1248, Training Accuracy: 92.27%, Validation Accuracy: 68.93%\n",
      "Epoch [46/51], Loss: 0.0631, Training Accuracy: 93.97%, Validation Accuracy: 73.02%\n",
      "Epoch [47/51], Loss: 0.0556, Training Accuracy: 95.48%, Validation Accuracy: 72.11%\n",
      "Epoch [48/51], Loss: 0.0622, Training Accuracy: 96.69%, Validation Accuracy: 72.34%\n",
      "Epoch [49/51], Loss: 0.0297, Training Accuracy: 97.18%, Validation Accuracy: 70.86%\n",
      "Epoch [50/51], Loss: 0.0938, Training Accuracy: 96.89%, Validation Accuracy: 71.88%\n",
      "Epoch [51/51], Loss: 0.1434, Training Accuracy: 97.52%, Validation Accuracy: 71.54%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 101\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for sequences, labels, lengths in train_loader:\n",
    "        sequences = sequences.unsqueeze(-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        hidden = model.init_hidden(sequences.size(0))\n",
    "        \n",
    "        outputs, hidden = model(sequences, lengths, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels, lengths in val_loader:\n",
    "            sequences = sequences.unsqueeze(-1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            hidden = model.init_hidden(sequences.size(0))\n",
    "            \n",
    "            outputs, hidden = model(sequences, lengths, hidden)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.49%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sequences, labels, lengths in train_loader:\n",
    "        sequences = sequences.unsqueeze(-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        hidden = model.init_hidden(sequences.size(0))\n",
    "        \n",
    "        outputs, hidden = model(sequences, lengths, hidden)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Training Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 71.54%\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sequences, labels, lengths in val_loader:\n",
    "        sequences = sequences.unsqueeze(-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        hidden = model.init_hidden(sequences.size(0))\n",
    "        \n",
    "        outputs, hidden = model(sequences, lengths, hidden)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_767804/3727951408.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequence, dtype=torch.float), label\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "test_sequences = [torch.tensor(seq, dtype=torch.float) for seq in test_data]\n",
    "test_dataset = SequenceDataset([(seq, 0) for seq in test_sequences])  # Dummy labels for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sequences, _, lengths in test_loader:\n",
    "        sequences = sequences.unsqueeze(-1).to(device)\n",
    "        hidden = model.init_hidden(sequences.size(0))\n",
    "        \n",
    "        outputs, hidden = model(sequences, lengths, hidden)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(predictions)\n",
    "df.to_csv('predictions.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
